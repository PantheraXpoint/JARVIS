# Custom Configuration for Decision-Only Mode
# Uses custom API tasks and models
# Points to custom demo files with examples matching your API tasks

# Local LLM Configuration (Qwen 2.5 7B)
local:
  endpoint: http://localhost:8006  # Qwen server endpoint

# HuggingFace Configuration (for model availability checks)
huggingface:
  token: REPLACE_WITH_YOUR_HUGGINGFACE_TOKEN_HERE  # Or set HUGGINGFACE_ACCESS_TOKEN environment variable

# Development settings
dev: true  # Enable local LLM mode
debug: true  # Enable debug logging for testing
log_file: logs/debug.log

# Model configuration
model: qwen-2.5-7b-instruct  # Local model name (Instruct version)
use_completion: false  # Use chat/completions API

# Inference configuration - HUGGINGFACE ONLY (no local models needed)
inference_mode: huggingface  # Only check HuggingFace models (no local server required)
local_deployment: minimal  # Not used in huggingface mode, but set anyway
device: cuda:0  # Not needed for decision-only, but set anyway

# Custom Model API Configuration
model_api_base_url: http://143.248.55.143:8080  # Base URL for custom model server

# Performance settings
num_candidate_models: 5  # Show more candidates for better testing
max_description_length: 200  # Longer descriptions for better decision-making

# Network configuration
proxy:  # Optional: your proxy server "http://ip:port"
http_listen:
  host: 0.0.0.0
  port: 8004

# Local inference endpoint (not used in huggingface mode, but required by code)
local_inference_endpoint:
  host: localhost
  port: 8005

# Logit bias settings
logit_bias:
  parse_task: 0.1
  choose_model: 5

# Prompt templates - NEW PIPELINE-BASED TASK PLANNING FORMAT (stored as JSON files)
tprompt:
  parse_task: demos/system_prompt_parse_task.json  # System prompt with task definitions and rules
  choose_model: >-
    #2 Model Selection Stage: Given the user request and the parsed tasks, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The assistant should focus more on the description of the model and find the model that has the most potential to solve requests and tasks. Also, prefer models with local inference endpoints for speed and stability.
  response_results: >-
    #4 Response Generation Stage: With the task execution logs, the AI assistant needs to describe the process and inference results.

# Demo files - NEW PIPELINE-BASED FORMAT with sensys examples
demos_or_presteps:
  parse_task: demos/demo_parse_task_sensys_formatted.json  # Pipeline DAG format with scenario â†’ tasks
  choose_model: demos/demo_choose_model_custom.json  # Same structure, task names will be replaced
  response_results: demos/demo_response_results_custom.json  # Custom demo showing execution plan format

# Prompt templates for different stages (user-facing prompts)
prompt:
  parse_task: demos/user_prompt_parse_task.json  # User prompt template with scenario input
  choose_model: >-
    Please choose the most suitable model from {{metas}} for the task {{task}}. The output must be in a strict JSON format: {"id": "id", "reason": "your detail reasons for the choice"}.
  response_results: >-
    You are describing an EXECUTION PLAN, not actual results. The processes show the planned workflow with selected models. Describe ONLY the execution plan: (1) Execution sequence - list which models will run first, second, third, etc. (task dependencies show order: task 1 depends on task 0 means task 0 runs first), (2) For each step, say what the model WILL analyze (e.g., "Step 1: The firedetect model will analyze the image to detect fire", "Step 2: The retina1face model will analyze the image to detect faces"), (3) Explain how the sequence solves the request. CRITICAL: Do NOT make up results like "fire detected" or "gender is male". Do NOT use past tense ("detected", "classified"). Use future tense only ("will detect", "will classify", "will analyze"). Do NOT provide a final answer to the user's question. Do NOT say "based on inference results" - say "based on the execution plan". Describe only what WILL happen when executed, not what has already happened.

