# Custom Configuration for Decision-Only Mode
# Uses custom API tasks and models
# Points to custom demo files with examples matching your API tasks

# Local LLM Configuration (Qwen 2.5 7B)
local:
  endpoint: http://localhost:8006  # Qwen server endpoint

# HuggingFace Configuration (for model availability checks)
huggingface:
  token: REPLACE_WITH_YOUR_HUGGINGFACE_TOKEN_HERE  # Or set HUGGINGFACE_ACCESS_TOKEN environment variable

# Development settings
dev: true  # Enable local LLM mode
debug: true  # Enable debug logging for testing
log_file: logs/debug.log

# Model configuration
model: qwen-2.5-7b-instruct  # Local model name (Instruct version)
use_completion: false  # Use chat/completions API

# Inference configuration - HUGGINGFACE ONLY (no local models needed)
inference_mode: huggingface  # Only check HuggingFace models (no local server required)
local_deployment: minimal  # Not used in huggingface mode, but set anyway
device: cuda:0  # Not needed for decision-only, but set anyway

# Custom Model API Configuration
model_api_base_url: http://143.248.55.143:8080  # Base URL for custom model server

# Performance settings
num_candidate_models: 5  # Show more candidates for better testing
max_description_length: 200  # Longer descriptions for better decision-making

# Network configuration
proxy:  # Optional: your proxy server "http://ip:port"
http_listen:
  host: 0.0.0.0
  port: 8004

# Local inference endpoint (not used in huggingface mode, but required by code)
local_inference_endpoint:
  host: localhost
  port: 8005

# Logit bias settings
logit_bias:
  parse_task: 0.1
  choose_model: 5

# Prompt templates - DYNAMIC TASK LIST (loaded from custom API)
tprompt:
  parse_task: >-
    #1 Task Planning Stage: The AI assistant can parse user input to several tasks: [{"task": task, "id": task_id, "dep": dependency_task_id, "args": {"image": image_url}}]. The "dep" field denotes the ids of the previous prerequisite tasks that must run first for logical ordering (e.g., object-detection-general before face-detection). CRITICAL: All tasks MUST use the EXACT SAME original image path provided by the user. DO NOT use "<GENERATED>" tags - they are NOT applicable here. All tasks analyze the same image and produce text outputs (labels, classifications, bounding boxes). The "args" field must only contain "image" with the original image path (e.g., "/examples/a.jpg" or "public/a.jpg"). NEVER use "<GENERATED>-id" in image paths. The task MUST be selected from the following options: [TASKS_LIST_WILL_BE_REPLACED_DYNAMICALLY]. There may be multiple tasks of the same type. Think step by step about all the tasks needed to resolve the user's request. Parse out as few tasks as possible while ensuring that the user request can be resolved. Pay attention to the dependencies and order among tasks. If the user input can't be parsed, you need to reply empty JSON [].
  choose_model: >-
    #2 Model Selection Stage: Given the user request and the parsed tasks, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The assistant should focus more on the description of the model and find the model that has the most potential to solve requests and tasks. Also, prefer models with local inference endpoints for speed and stability.
  response_results: >-
    #4 Response Generation Stage: With the task execution logs, the AI assistant needs to describe the process and inference results.

# Demo files - CUSTOM VERSIONS with your API tasks
demos_or_presteps:
  parse_task: demos/demo_parse_task_custom.json  # Custom examples with your tasks
  choose_model: demos/demo_choose_model_custom.json  # Same structure, task names will be replaced
  response_results: demos/demo_response_results_custom.json  # Custom demo showing execution plan format

# Prompt templates for different stages
prompt:
  parse_task: >-
    The chat log [ {{context}} ] may contain the resources I mentioned. Now I input { {{input}} }. Pay attention to the input and output types of tasks and the dependencies between tasks. Consider logical ordering: for example, object-detection-general should typically run before face-detection or gender-classification, as detecting objects first helps locate people in the image.
  choose_model: >-
    Please choose the most suitable model from {{metas}} for the task {{task}}. The output must be in a strict JSON format: {"id": "id", "reason": "your detail reasons for the choice"}.
  response_results: >-
    You are describing an EXECUTION PLAN, not actual results. The processes show the planned workflow with selected models. Describe ONLY the execution plan: (1) Execution sequence - list which models will run first, second, third, etc. (task dependencies show order: task 1 depends on task 0 means task 0 runs first), (2) For each step, say what the model WILL analyze (e.g., "Step 1: The firedetect model will analyze the image to detect fire", "Step 2: The retina1face model will analyze the image to detect faces"), (3) Explain how the sequence solves the request. CRITICAL: Do NOT make up results like "fire detected" or "gender is male". Do NOT use past tense ("detected", "classified"). Use future tense only ("will detect", "will classify", "will analyze"). Do NOT provide a final answer to the user's question. Do NOT say "based on inference results" - say "based on the execution plan". Describe only what WILL happen when executed, not what has already happened.

